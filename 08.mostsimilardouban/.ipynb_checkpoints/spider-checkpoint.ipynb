{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! python3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"mymovie.txt\", \"rb\") as fp:\n",
    "    movie_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_string(item):    \n",
    "    paragraphs = []\n",
    "    for x in item:\n",
    "        paragraphs.append(str(x))\n",
    "    return paragraphs[3]\n",
    "\n",
    "def get_url(item):  \n",
    "    pattern3 = re.compile('<a class=.+href=\\\"(.+)/\\\">',re.S)\n",
    "    url = re.findall(pattern3,item)\n",
    "    return url\n",
    "\n",
    "def find_next(soup):\n",
    "    next = str(soup.find_all('a', class_=\"next\")[0])\n",
    "    print(next)\n",
    "    pattern = re.compile('<a class=\"next\\\".+href=\\\"(\\S+)\\\">',re.S)\n",
    "    url = re.findall(pattern,next)[0]\n",
    "    url.split(\";\")\n",
    "    new = \"\"\n",
    "    count = 0\n",
    "    for i in url.split(\";\"):\n",
    "        if count == 0:\n",
    "            new = i\n",
    "        else:\n",
    "            new = new + \"&\" + i\n",
    "        count = count + 1\n",
    "    return initial_url+ new.replace('amp&','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cookies = {}\n",
    "raw_cookies=input(\"Enter the cookies:\")\n",
    "for line in raw_cookies.split(';'):\n",
    "    key,value=line.split('=',1)#1代表只分一次，得到两个数据\n",
    "    cookies[key[1:]]=value\n",
    "\n",
    "user_url = set()\n",
    "for movie in movie_list:\n",
    "    url = movie + \"/comments\"\n",
    "    initial_url = url\n",
    "    print(\"开始爬取电影的评分用户地址: \" + url) \n",
    "    for i in range(0,100000,1):\n",
    "        r= requests.get(url,cookies=cookies)\n",
    "        soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "        print(\"导入为soup\")\n",
    "        a = soup.find_all('h3')\n",
    "        url = find_next(soup,initial_url)\n",
    "        for item in a:\n",
    "            user_url.add(get_info(to_string(item)))\n",
    "        print(\"结果写入完成！\" + \"目前有数据条数：\" + str(len(user_url)))\n",
    "        time.sleep(3)\n",
    "        print(\"睡眠结束！再次爬行！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
